{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-8q8rRRWcp6"
   },
   "source": [
    "# TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism\n",
    "\n",
    "**Reprise des commentaires sur https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/**\n",
    "\n",
    "Code et tutoriel original ici: \n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "      <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/addons/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9n0dcDw1Wszw"
   },
   "source": [
    "## Overview\n",
    "This notebook gives a brief introduction into the ***Sequence to Sequence Model Architecture***\n",
    "In this noteboook you broadly cover four essential topics necessary for Neural Machine Translation:\n",
    "\n",
    "\n",
    "* **Data cleaning**\n",
    "* **Data preparation**\n",
    "* **Neural Translation Model with Attention**\n",
    "* **Final Translation with ```tf.addons.seq2seq.BasicDecoder``` and ```tf.addons.seq2seq.BeamSearchDecoder```** \n",
    "\n",
    "The basic idea behind such a model though, is only the encoder-decoder architecture. These networks are usually used for a variety of tasks like text-summerization, Machine translation, Image Captioning, etc. This tutorial provideas a hands-on understanding of the concept, explaining the technical jargons wherever necessary. You focus on the task of Neural Machine Translation (NMT) which was the very first testbed for seq2seq models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpySVYWJhxaV"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Moi j'ai installé tf addons par `pip install tensorflow-addons==0.13.0` (ET NON PAS `conda install -c esri tensorflow-addons`). Voir les compatibilités [sur le github de tensorflow_addons](https://github.com/tensorflow/addons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_kxfdP4hJUPB"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons==0.11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:36:06) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii_vg-XNXTil"
   },
   "source": [
    "# Step 1: Import the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PvRnGWnvXm6l"
   },
   "outputs": [],
   "source": [
    "# Untar the dataset\n",
    "# !unzip 'fra-eng.zip' # under linux\n",
    "\n",
    "# Get the txt file which has English -> French translation\n",
    "path_to_file  = './word2vec_docs_scol_traités/fra.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFKB2c_tX4wU"
   },
   "source": [
    "# Step 2: Preprocess the data\n",
    "\n",
    "The dataset has Unicode characters, which have to be normalized.\n",
    "\n",
    "Moreover, all the tokens in the sequences have to be cleaned using the regular expressions library.\n",
    "\n",
    "Remove unwanted spaces, include a space between every word and the punctuation following it (to differentiate between both), replace unwanted characters with spaces, and append `<start>` and `<end>` tokens to specify the start and end of a sequence.\n",
    "\n",
    "Encapsulate the unicode conversion in a function `unicode_to_ascii()` and sequence preprocessing in a function preprocess_sentence().\n",
    "\n",
    "## Data Cleaning and Data Preparation \n",
    "\n",
    "You'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "---\n",
    "      May I borrow this book?    ¿Puedo tomar prestado este libro?\n",
    "---\n",
    "\n",
    "\n",
    "There are a variety of languages available, but you'll use the English-Spanish dataset. After downloading the dataset, here are the steps you'll take to prepare the data:\n",
    "\n",
    "\n",
    "1. Add a start and end token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a Vocabulary with word index (mapping from word → id) and reverse word index (mapping from id → word).\n",
    "5. Pad each sentence to a maximum length. (Why? you need to fix the maximum length for the inputs to recurrent encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> go . va ! cc by . france attribution tatoeba . org cm wittydev <end> \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['<start> go . <end>', '<start> va ! <end>'],\n",
       " ['<start> go . <end>', '<start> marche . <end>'],\n",
       " ['<start> go . <end>', '<start> bouge ! <end>'],\n",
       " ['<start> hi . <end>', '<start> salut ! <end>'],\n",
       " ['<start> hi . <end>', '<start> salut . <end>']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "import re\n",
    "\n",
    "# Convert the unicode sequence to ascii\n",
    "def unicode_to_ascii(s):\n",
    "\n",
    "  # Normalize the unicode string and remove the non-spacking mark\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Preprocess the sequence\n",
    "def preprocess_sentence(w):\n",
    "  # Clean the sequence\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # Create a space between word and the punctuation following it\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # Replace everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # Add a start and stop token to detect the start and end of the sequence\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w\n",
    "\n",
    "\n",
    "\n",
    "phrases = io.open(path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "\n",
    "for phrase in phrases[:1]:\n",
    "    print(preprocess_sentence(phrase), '\\n')\n",
    "    \n",
    "[[preprocess_sentence(w) for w in l.split('\\t')[:2]] for l in phrases[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JMAHz7kJXc5N"
   },
   "outputs": [],
   "source": [
    "# Create the Dataset\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    # Loop through lines (sequences) and extract the English and French sequences. Store them as a word-pair\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t', 2)[:-1]]  for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs)\n",
    "\n",
    "en, fra = create_dataset(path_to_file, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "3\n",
      "('<start> go . <end>', '<start> go . <end>', '<start> go . <end>')\n",
      "('<start> va ! <end>', '<start> marche . <end>', '<start> bouge ! <end>')\n"
     ]
    }
   ],
   "source": [
    "print(type(en))\n",
    "print(len(en))\n",
    "print(en)\n",
    "print(fra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now tokenize the sequences. Tokenization is the mechanism of creating an internal vocabulary comprising English and French tokens (i.e. words), converting the tokens (or, in general, sequences) to integers, and padding them all to make the sequences possess the same length. All in all, tokenization facilitates the model training process.\n",
    "\n",
    "Create a function `tokenize()` to encapsulate all the above-mentioned requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequences to tokenizers\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  \n",
    "  # Convert sequences into internal vocab\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  # Convert internal vocab to numbers\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  # Pad the tensors to assign equal length to all the sequences\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenized dataset by calling the `create_dataset()` and `tokenize()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_dataset(path, num_examples=None):\n",
    " \n",
    "  # Create dataset (targ_lan = English, inp_lang = French)\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "  # targ_lang, inp_lang are tuples, each containing the ordered sentences\n",
    "\n",
    "  # Tokenize the sequences\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the number of data samples required to train the model. Employing the whole dataset will consume a lot more time for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider 50k examples\n",
    "num_examples = 10000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "# inp_lang, targ_lang are tokenizers whose attribute index_word is a dictionary matching indices (keys: integers) to their corresponding words (values: string words)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tenseur\n",
      "<class 'numpy.ndarray'>\n",
      "(10000, 15)\n",
      "tokenizer\n",
      "<class 'keras_preprocessing.text.Tokenizer'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(\"tenseur\")\n",
    "print(type(input_tensor))\n",
    "print(np.shape(input_tensor))\n",
    "input_tensor[0]\n",
    "print(\"tokenizer\")\n",
    "print(type(inp_lang))\n",
    "print(type(inp_lang.index_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `max_length` of both the input and target tensors is essential to determine every sequence's maximum padded length.\n",
    "# Step 4: Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "8000 8000 2000 2000\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and validation sets using an 80/20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "print(type(input_tensor_train), type(target_tensor_train))\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the mapping that’s been created between the tokens of the sequences and the indices. <br /> `inp_lang.index_word` is a dictionary whose keys are integers (indices of words) and values are strings (words of the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "24 ----> tu\n",
      "31 ----> es\n",
      "499 ----> idiote\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "6 ----> you\n",
      "12 ----> re\n",
      "335 ----> silly\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "# Show the mapping b/w word index and language tokenizer\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor: # t est un entier élément du tenseur\n",
    "    if t != 0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "      \n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Initialize the Model Parameters\n",
    "\n",
    "With the dataset in hand, start initializing the model parameters.\n",
    "\n",
    "* `BUFFER_SIZE`: Total number of input/target samples. In our model, it’s 40,000.\n",
    "* `BATCH_SIZE`: Length of the training batch.\n",
    "* `steps_per_epoch`: The number of steps per epoch. Computed by dividing BUFFER_SIZE by BATCH_SIZE.\n",
    "* `embedding_dim`: Number of nodes in the embedding layer.\n",
    "* `units: Hidden` units in the network.\n",
    "* `vocab_inp_size`: Length of the input (French) vocabulary.\n",
    "* `vocab_tar_size`: Length of the output (English) vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EIW4NVBmJ25k"
   },
   "outputs": [],
   "source": [
    "# Essential model parameters\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE # = num_examples//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1 # pourquoi +1 ?\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, call the `tf.data.Dataset` API and create a proper dataset. <br /> Documentation if the `from_tensor_slices`: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices\n",
    "\n",
    "The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "w2lCTy4vKOkB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characteristics of dataset\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'> 125\n",
      "\n",
      "characteristics of list(dataset.as_numpy_iterator())\n",
      "<class 'list'> 125 (= steps_per_epoch)\n",
      "<class 'tuple'> 2\n",
      "<class 'numpy.ndarray'> (64, 15) (= BATCH_SIZE,max_length_inp)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(f\"characteristics of dataset\")\n",
    "print(type(dataset), len(dataset))\n",
    "dataset_iterator_list = list(dataset.as_numpy_iterator())\n",
    "\n",
    "print(\"\\ncharacteristics of list(dataset.as_numpy_iterator())\")\n",
    "\n",
    "print(type(dataset_iterator_list),len(dataset_iterator_list), \"(= steps_per_epoch)\")\n",
    "print(type(dataset_iterator_list[0]), len(dataset_iterator_list[0])) \n",
    "print(type(dataset_iterator_list[0][0]), np.shape(dataset_iterator_list[0][0]), \"(= BATCH_SIZE,max_length_inp)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "Validate the shapes of the input and target batches of the newly-created dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 15]), TensorShape([64, 8]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of input and target batches\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64 (first dimension) is the batch size\n",
    "19 and 11 denote the maximum padded lengths of the input (French) and target (English) sequences.\n",
    "\n",
    "# Step 6: Encoder Class\n",
    "\n",
    "The first step in creating an encoder-decoder sequence-to-sequence model (with an attention mechanism) is creating an encoder. For the application at hand, create an encoder with an embedding layer followed by a GRU (Gated Recurrent Unit) layer. The input goes through the embedding layer first and then into the GRU layer. The GRU layer outputs both the encoder network output and the hidden state.\n",
    "\n",
    "Enclose the model’s `__init__()` and `call()` methods in a class Encoder.\n",
    "\n",
    "In the method, `__init__()`, initializes the batch size and encoding units. Add an embedding layer that accepts `vocab_size` as the input dimension and `embedding_dim` as the output dimension. Also, add a GRU layer that accepts units (dimensionality of the output space) and the first hidden dimension.\n",
    "\n",
    "In the method `call()`, define the forward propagation that has to happen through the encoder network.\n",
    "\n",
    "Moreover, define a method `initialize_hidden_state()` to initialize the hidden state with the dimensions `batch_size` and units.\n",
    "\n",
    "Add the following code as part of your Encoder class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "# Encoder class\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "\n",
    "    # Embed the vocab to a dense embedding \n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    # GRU Layer\n",
    "    # glorot_uniform: Initializer for the recurrent_kernel weights matrix, \n",
    "    # used for the linear transformation of the recurrent state\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  # Encoder network comprises an Embedding layer followed by a GRU layer\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "    return output, state\n",
    "\n",
    "  # To initialize the hidden state\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the encoder class to check the shapes of the encoder output and hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "60gSVh05Jl6l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output       shape: (batch size, sequence length, units) (64, 15, 1024)\n",
      "Encoder Hidden state shape: (batch size, units)                  (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder Output       shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units)                  {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('encoder.h5', save_format=\"tf\")\n",
    "# NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAAA8CAYAAAC+ej5cAAAABmJLR0QA/wD/AP+gvaeTAAADaklEQVR4nO2cu07rQBCGfx94CF5q3UHBE2xKeiMKKEGURKZEkS3KuCZt2iAKBAWSUxFX8QOgOQXaxdfEdm4MmU+i8DDeHe9n4wkGO0REENjyb9cFCKshApkjApkjAplzWAx8fn7i7OwMX19fu6hHqOHg4AC3t7c4OjrKxUtX4Gg0QhiGWytMaEYYhhiNRqV46Qo0PD4+brQgoR2O41TG5R7IHBHIHBHIHBHIHBHIHBHIHBHIHBHIHBHIHBHIHBHIHBHIHBHIHBHInL0RmCQJwjCE67q7LmWt1D4P/GtcXFyg3+9vdc40TfH6+oqXlxdEUYThcLj2OfZG4N3d3dYFXl9fAwCurq42NsfeCNwFl5eXADYrcG33wCRJcHNzA8dx4Lqu/fuN4r0niiKbM51Oc2OkaYowDOE4DhzHwf39fWmeqpwkSRbmua6L9/f31nVHUQTXdZGmKXq9Hs7Pz1dao41ABQaDAVWEFzKbzUgpRUEQEBHR09MTAaDJZEJKKQJAAGg8HhMRURzHBIC01rlxlFLkeZ7d1lrntk2O7/u5eZVSNJ/PS3laaxsPgsDW0aXuyWRSqrcpxXm7jjEYDMrxYqCLQLM4xQnN4lcdQDFmxpjNZjY2Ho9JKWW3zQIXcwBYCUREw+GQANDb25uNzefz2jmX1V08Odry6wVmz9bil5l8mUAzxiK01qUcIyYruipv0Zxt6u7Crxe4rMAmApscZF1O07HazslB4Fo/yNc1Ck1QSgEAnp+fl+ZUNS1a685zr1L3rlmLQN/3AQAPDw9I0xTAT3fXFCOn3+/bMabTKXq9ns05PT0FAHx8fNiYyT0+Pi7Vs+hkWFfdO6d4SXbtQlFxH4njOPc90wxkGwrTkJiOMLu/1rrUiJiu0+wXBEGpOzRdrlKK4jgmop8GyIzbpu5VyB7rKs0QNnkPJPpeNM/z7AKZhSsuTl2M6HtBzRie5+XkZXN837f7BkFQuTBxHNtmRmud+8iQ7WKb1J1tkNpQdXJ0PSE2LlDYLHUC9+ZpxF9FBDJHfpndkrp/8ypCW3r5hwhsybbENEV+hDJHBDJHBDJHBDJHBDJHBDJHBDJHBDJHBDJHBDJHBDJHBDJHBDKn9mnEycnJNusQOuJQ4fmIvPD1d1L3wteSQIEXcg9kjghkjghkjghkzn9GKOEadHEb4gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(encoder, to_file='encoder.png', show_shapes=True, show_dtype=True,\n",
    "    show_layer_names=True, rankdir='TB', expand_nested=True, dpi=96,\n",
    "    layer_range=None, show_layer_activations=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Attention Mechanism Class\n",
    "\n",
    "This step captures the attention mechanism.\n",
    "\n",
    "* Compute the sum (or product) of the encoder’s outputs and decoder states.\n",
    "* Pass the generated output through a fully-connected network.\n",
    "* Apply softmax activation to the output. This gives the attention weights.\n",
    "* Create the context vector by computing the weighted sum of attention weights and encoder’s outputs.\n",
    "\n",
    "Everything thus far needs to be captured in a class `BahdanauAttention`. **Bahdanau Attention** is also called the **“Additive Attention”**, a **Soft Attention** technique. As this is additive attention, we do the sum of the encoder’s outputs and decoder hidden state (as mentioned in the first step).\n",
    "\n",
    "This class has to have `__init__()` and `call()` methods.\n",
    "\n",
    "- In the `__init__()` method, initialize three Dense layers: one for the decoder state ('units' is the size), another for the encoder’s outputs ('units' is the size), and the other for the fully-connected network (one node).\n",
    "\n",
    "- In the `call()` method, initialize the decoder state ($s_0$) by taking the final encoder hidden state. Pass the generated decoder hidden state through one dense layer. Also, plug the encoder’s outputs through the other dense layer. Add both the outputs, encase them in a $\\tanh$ activation and plug them into the fully-connected layer. This fully-connected layer has one node; thus, the final output has the dimensions `batch_size * max_length of the sequence * 1`.\n",
    "\n",
    "Later, apply $\\text{softmax}$ on the output of the fully-connected network to generate the attention weights.\n",
    "\n",
    "Compute the `context_vector` by performing a weighted sum of the attention weights and the encoder’s outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "# Attention Mechanism\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the shapes of the Attention weights and its output.\n",
    "`sample_hidden` here is the hidden state of the encoder, and `sample_output` denotes the encoder’s outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DaiO0Z6_Ml1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 19, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Decoder Class\n",
    "\n",
    "This step encapsulates the decoding mechanism. The Decoder class has to have two methods: `__init__()` and `call()`.\n",
    "\n",
    "In the `__init__()` method, initialize the batch size, decoder units, embedding dimension, GRU layer, and a Dense layer. Also, create an instance of the `BahdanauAttention` class.\n",
    "\n",
    "In the `call()` method:\n",
    "\n",
    "* Call the attention forward propagation and capture the context vector and attention weights.\n",
    "* Send the target token through an embedding layer.\n",
    "* Concatenate the embedded output and context vector.\n",
    "* Plug the output into the GRU layer and then into a fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder class\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # Used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # x shape == (batch_size, 1)\n",
    "    # hidden shape == (batch_size, max_length)\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "    # context_vector shape == (batch_size, hidden_size)\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the decoder output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 5938)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  2608640   \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,546,944\n",
      "Trainable params: 6,546,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  1520128   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 multiple                  7084032   \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  6086450   \n",
      "                                                                 \n",
      " bahdanau_attention_1 (Bahda  multiple                 2100225   \n",
      " nauAttention)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,790,835\n",
      "Trainable params: 16,790,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "# Step 9: Define the optimizer and the loss function\n",
    "Define the optimizer and loss functions.\n",
    "\n",
    "As the input sequences are being padded with zeros, nullify the loss when there’s a zero in the `real` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss functions\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "# Loss function\n",
    "def loss_function(real, pred):\n",
    "\n",
    "  # Take care of the padding. Not all sequences are of equal length.\n",
    "  # If there's a '0' in the sequence, the loss is being nullified\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "# Step 10: Train the Model\n",
    "Checkpoint your model’s weights during training. This helps in the automatic retrieval of the weights while evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,encoder=encoder,decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the training procedure. First, call the encoder class and procure the encoder outputs and final hidden state. Initialize the decoder input to have the `<start>` token spread across all the input sequences (indicated using the `BATCH_SIZE`). Use the teacher forcing technique to iterate over all decoder states by feeding the target as the next input. This loop continues until every token in the target sequence (English) is visited.\n",
    "\n",
    "Call the decoder class with decoder input, decoder hidden state, and encoder’s outputs. Procure the decoder output and hidden state. Compute the loss by comparing the real against the predicted value of the target. Fetch the target token and feed it to the next decoder state (concerning the successive target token). Also, make a note that the target decoder hidden state will be the next decoder hidden state.\n",
    "\n",
    "After the teacher forcing technique gets finished, compute the batch loss, and run the optimizer to update the model's variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  # tf.GradientTape() -- record operations for automatic differentiation\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    # dec_hidden is used by attention, hence is the same enc_hidden\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    # <start> token is the initial decoder input\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "\n",
    "      # Pass enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      # Compute the loss\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # Use teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  # As this function is called per batch, compute the batch_loss\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  # Get the model's variables\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  # Compute the gradients\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  # Update the variables of the model/network\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pey8eb9piMMg"
   },
   "source": [
    "Now initialize the actual training loop. Run your loop over a specified number of epochs. First, initialize the encoder hidden state using the method `initialize_hidden_state()`. Loop through the dataset one batch at a time (per epoch). Call the `train_step()` method per batch and compute the loss. Continue until all the epochs have been covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ddefjBMa3jF0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.7148\n",
      "Epoch 1 Batch 100 Loss 2.1596\n",
      "Epoch 1 Batch 200 Loss 1.8383\n",
      "Epoch 1 Batch 300 Loss 1.7680\n",
      "Epoch 1 Batch 400 Loss 1.5964\n",
      "Epoch 1 Batch 500 Loss 1.6257\n",
      "Epoch 1 Batch 600 Loss 1.5310\n",
      "Epoch 1 Loss 1.8884\n",
      "Time taken for 1 epoch 122.8930013179779 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.5565\n",
      "Epoch 2 Batch 100 Loss 1.4449\n",
      "Epoch 2 Batch 200 Loss 1.4419\n",
      "Epoch 2 Batch 300 Loss 1.4422\n",
      "Epoch 2 Batch 400 Loss 1.3437\n",
      "Epoch 2 Batch 500 Loss 1.2759\n",
      "Epoch 2 Batch 600 Loss 1.3462\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # Initialize the hidden state\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  # Loop through the dataset\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "\n",
    "    # Call the train method\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "\n",
    "    # Compute the loss (per batch)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # Save (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  # Output the loss observed until that epoch\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  \n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "# Step 11: Test the Model\n",
    "## Use tf-addons BasicDecoder for decoding\n",
    "\n",
    "Now define your model evaluation procedure. First, take the sentence given by the user into consideration. This has to be given in the French language. The model now has to convert the sentence from French to English.\n",
    "\n",
    "Initialize an empty attention plot to be plotted later on with `max_length_target` on the Y-axis, and max_length_input on the X-axis.\n",
    "\n",
    "Preprocess the sentence and convert it into tensors.\n",
    "\n",
    "Then plug the sentence into the model.\n",
    "\n",
    "Initialize an empty hidden state which is to be used while initializing an encoder. Usually, the `initialize_hidden_state()` method in the encoder class gives the hidden state having the dimensions `batch_size * hidden_units`. Now, as the batch size is 1, the initial hidden state has to be manually initialized.\n",
    "\n",
    "Call the encoder class and procure the encoder outputs and final hidden state.\n",
    "\n",
    "By looping over `max_length_targ`, call the decoder class wherein the `dec_input` is the `<start>` token, `dec_hidden` state is the encoder hidden state, and `enc_out` is the encoder’s outputs. Procure the decoder output, hidden state, and attention weights.\n",
    "\n",
    "Create a plot using the attention weights. Fetch the predicted token with the maximum attention. Append the token to the result and continue until the `<end>` token is reached.\n",
    "\n",
    "The next decoder input will be the previously predicted index (concerning the token).\n",
    "\n",
    "Add the following code as part of the `evaluate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate function -- similar to the training loop\n",
    "def evaluate(sentence):\n",
    "\n",
    "  # Attention plot (to be plotted later on) -- initialized with max_lengths of both target and input\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  # Preprocess the sentence given\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # Fetch the indices concerning the words in the sentence and pad the sequence\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  # Convert the inputs to tensors\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  # Loop until the max_length is reached for the target lang (ENGLISH)\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # Store the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    # Get the prediction with the maximum attention\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    # Append the token to the result\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    # If <end> token is reached, return the result, input, and attention plot\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # The predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n250XbnjOaqP"
   },
   "source": [
    "# Step 12: Plot and Predict\n",
    "\n",
    "Define the `plot_attention()` function to plot the attention statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function `translate()` which internally calls the `evaluate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSx2iM36EZQZ"
   },
   "outputs": [],
   "source": [
    "# Translate function (which internally calls the evaluate function)\n",
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the saved checkpoint to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2c0e864bf40>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restore the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ou es tu maintenant <end>\n",
      "Predicted translation: are we ready ? <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp/ipykernel_11340/124968884.py:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp/ipykernel_11340/124968884.py:13: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u\"ou es tu maintenant ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "networks_seq2seq_nmt.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
